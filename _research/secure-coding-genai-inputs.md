---
layout: default
title: "Secure Coding vs. GenAI Inputs: An Empirical Audit"
description: "Evaluating the security quality of code snippets generated by AI, identifying vulnerable patterns, and defining guidelines for developers."
permalink: /research/secure-coding-genai-inputs/
---

# Secure Coding vs. GenAI Inputs
## An Empirical Audit of AI-Generated Vulnerabilities

---

> **PROJECT STATUS:** `PLANNED` ðŸŸ¡
> **DOMAIN:** Application Security (AppSec) Â· **TARGET:** GenAI Code Snippets
> **FOCUS:** Vulnerability Assessment Â· SAST/SCA Â· Developer Guidelines
> ![Security](https://img.shields.io/badge/AppSec-Vulnerability_Audit-red?style=flat-square) ![AI](https://img.shields.io/badge/AI-Generative-green?style=flat-square)

### âš¡ TL;DR

Developers increasingly rely on AI to generate boilerplate and logic. But how secure is this code "out of the box"? This research performs a systematic security audit of code produced by popular GenAI models, focusing on common vulnerability classes (OWASP Top 10).

### ðŸ”¬ Core Research Areas

1.  **Empirical Audit:** Generating and analyzing hundreds of code snippets for standard tasks (auth, DB queries, file handling).
2.  **Pattern Recognition:** Identifying recurring insecure patterns (e.g., hardcoded secrets, lack of input sanitization).
3.  **SAST Evaluation:** Running Static Application Security Testing tools against AI-generated code.
4.  **Remediation:** Developing "fix-it" prompts and guidelines for securely integrating GenAI code.

**Expected Outcome:** A catalog of "AI-native" vulnerabilities, statistical safety ratings for different prompting strategies, and a developer's guide to safe AI adoption.
